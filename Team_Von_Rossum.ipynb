{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# import census\n",
    "from census import Census\n",
    "from us import states\n",
    "# Census API Key\n",
    "from config import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carl's code\n",
    "\n",
    "# Read in the individual bike files, append a new column for month\n",
    "m_df1 = pd.read_csv(\"resources/Minneapolis/201804-niceride-tripdata.csv\")\n",
    "m_df1[\"Month\"] = \"April\"\n",
    "m_df2 = pd.read_csv(\"resources/Minneapolis/201805-niceride-tripdata.csv\")\n",
    "m_df2[\"Month\"] = \"May\"\n",
    "m_df3 = pd.read_csv(\"resources/Minneapolis/201806-niceride-tripdata.csv\")\n",
    "m_df3[\"Month\"] = \"June\"\n",
    "m_df4 = pd.read_csv(\"resources/Minneapolis/201807-niceride-tripdata.csv\")\n",
    "m_df4[\"Month\"] = \"July\"\n",
    "m_df5 = pd.read_csv(\"resources/Minneapolis/201808-niceride-tripdata.csv\")\n",
    "m_df5[\"Month\"] = \"August\"\n",
    "m_df6 = pd.read_csv(\"resources/Minneapolis/201809-niceride-tripdata.csv\")\n",
    "m_df6[\"Month\"] = \"September\"\n",
    "m_df7 = pd.read_csv(\"resources/Minneapolis/201810-niceride-tripdata.csv\")\n",
    "m_df7[\"Month\"] = \"October\"\n",
    "m_df8 = pd.read_csv(\"resources/Minneapolis/201811-niceride-tripdata.csv\")\n",
    "m_df8[\"Month\"] = \"November\"\n",
    "\n",
    "# create a new data frame and append individual data frames together\n",
    "m_df = pd.DataFrame()\n",
    "m_df = m_df1.append(m_df2).append(m_df3).append(m_df4).append(m_df5).append(m_df6).append(m_df7).append(m_df8)\n",
    "m_df.to_csv('census_output/m_bike.csv')\n",
    "# m_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the individual bike files, append a new column for month\n",
    "b_df1 = pd.read_csv(\"resources/Boston/201801_hubway_tripdata.csv\")\n",
    "b_df1[\"Month\"] = \"January\"\n",
    "b_df2 = pd.read_csv(\"resources/Boston/201802_hubway_tripdata.csv\")\n",
    "b_df2[\"Month\"] = \"February\"\n",
    "b_df3 = pd.read_csv(\"resources/Boston/201803_hubway_tripdata.csv\")\n",
    "b_df3[\"Month\"] = \"March\"\n",
    "b_df4 = pd.read_csv(\"resources/Boston/201804-hubway-tripdata.csv\")\n",
    "b_df4[\"Month\"] = \"April\"\n",
    "b_df5 = pd.read_csv(\"resources/Boston/201805-bluebikes-tripdata.csv\")\n",
    "b_df5[\"Month\"] = \"May\"\n",
    "b_df6 = pd.read_csv(\"resources/Boston/201806-bluebikes-tripdata.csv\")\n",
    "b_df6[\"Month\"] = \"June\"\n",
    "b_df7 = pd.read_csv(\"resources/Boston/201807-bluebikes-tripdata.csv\")\n",
    "b_df7[\"Month\"] = \"July\"\n",
    "b_df8 = pd.read_csv(\"resources/Boston/201808-bluebikes-tripdata.csv\")\n",
    "b_df8[\"Month\"] = \"August\"\n",
    "b_df9 = pd.read_csv(\"resources/Boston/201809-bluebikes-tripdata.csv\")\n",
    "b_df9[\"Month\"] = \"September\"\n",
    "b_df10 = pd.read_csv(\"resources/Boston/201810-bluebikes-tripdata.csv\")\n",
    "b_df10[\"Month\"] = \"October\"\n",
    "b_df11 = pd.read_csv(\"resources/Boston/201811-bluebikes-tripdata.csv\")\n",
    "b_df11[\"Month\"] = \"November\"\n",
    "b_df12 = pd.read_csv(\"resources/Boston/201812-bluebikes-tripdata.csv\")\n",
    "b_df12[\"Month\"] = \"December\"\n",
    "\n",
    "# create a new data frame and append individual data frames together\n",
    "b_df = pd.DataFrame()\n",
    "b_df = b_df1.append(b_df2).append(b_df3).append(b_df4).append(b_df5).append(b_df6).append(b_df7).append(b_df8).append(b_df9).append(b_df10).append(b_df11).append(b_df12)\n",
    "b_df.to_csv('census_output/b_bike.csv')\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df1 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df1[\"Month\"] = \"January\"\n",
    "p_df2 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df2[\"Month\"] = \"February\"\n",
    "p_df3 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df3[\"Month\"] = \"March\"\n",
    "p_df4 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df4[\"Month\"] = \"April\"\n",
    "p_df5 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df5[\"Month\"] = \"May\"\n",
    "p_df6 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df6[\"Month\"] = \"June\"\n",
    "p_df7 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df7[\"Month\"] = \"July\"\n",
    "p_df8 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df8[\"Month\"] = \"August\"\n",
    "p_df9 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df9[\"Month\"] = \"September\"\n",
    "p_df10 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df10[\"Month\"] = \"October\"\n",
    "p_df11 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df11[\"Month\"] = \"November\"\n",
    "p_df12 = pd.read_csv(\"resources/Portland/2018_01.csv\")\n",
    "p_df12[\"Month\"] = \"December\"\n",
    "\n",
    "# create a new data frame and append individual data frames together\n",
    "p_df = pd.DataFrame()\n",
    "p_df = p_df1.append(p_df2).append(p_df3).append(p_df4).append(p_df5).append(p_df6).append(p_df7).append(p_df8).append(p_df9).append(p_df10).append(p_df11).append(p_df12)\n",
    "p_df.to_csv('census_output/p_bike.csv')\n",
    "p_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start bringing in census data. We want the data per census tract of the counties that contain Minneapolis, Boston and Portland\n",
    "\n",
    "# Get census data for Minneapolis\n",
    "c = Census(api_key, year=2017)\n",
    "m_census_data = c.acs5.get(('NAME','B01003_001E', 'B19013_001E', 'B17001_002E', 'B08301_001E', 'B08301_003E', 'B08101_041E',), geo={'for': 'tract:*',\n",
    "                       'in': 'state:{} county:053'.format(states.MN.fips)}) #  county:053 is Hennepin County\n",
    "# Convert to DataFrame\n",
    "m_census_pd = pd.DataFrame(m_census_data)\n",
    "\n",
    "# Column Renaming\n",
    "m_census_pd = m_census_pd.rename(columns={\"B01003_001E\": \"Population\",\n",
    "                                      \"B19013_001E\": \"Median Household Income\",\n",
    "                                      \"B17001_002E\": \"Poverty count\",\n",
    "                                      \"B08301_001E\": \"Commuting count\",\n",
    "                                      \"B08301_003E\": \"Commuting by car count\",\n",
    "                                      \"B08101_041E\": \"Commuting OTHER count\",\n",
    "                                      \"NAME\": \"Name\", \"tract\": \"Census Tract\"})\n",
    "\n",
    "# Add in Poverty Rate (Poverty Count / Population)\n",
    "m_census_pd[\"Poverty Rate\"] = 100 * \\\n",
    "    m_census_pd[\"Poverty count\"].astype(\n",
    "        int) / m_census_pd[\"Population\"].astype(int)\n",
    "\n",
    "# Calculate commute by car\n",
    "m_census_pd[\"Car Rate\"] = 100 * \\\n",
    "    m_census_pd[\"Commuting by car count\"].astype(\n",
    "        int) / m_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate commute by OTHER\n",
    "m_census_pd[\"Commute OTHER rate\"] = 100 * \\\n",
    "    m_census_pd[\"Commuting OTHER count\"].astype(\n",
    "        int) / m_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate GEOID <= this is used to join to census geography\n",
    "m_census_pd[\"GEOID\"] = m_census_pd[\"state\"].astype(str)+m_census_pd[\"county\"]+m_census_pd[\"Census Tract\"]\n",
    "# m_census_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get census data for Boston\n",
    "b_census_data = c.acs5.get(('NAME','B01003_001E', 'B19013_001E', 'B17001_002E', 'B08301_001E', 'B08301_003E', 'B08101_041E',), geo={'for': 'tract:*',\n",
    "                       'in': 'state:{} county:025'.format(states.MA.fips)}) #  county:025 is Suffolk County\n",
    "\n",
    "# Convert to DataFrame\n",
    "b_census_pd = pd.DataFrame(b_census_data)\n",
    "\n",
    "# Column Reordering\n",
    "b_census_pd = b_census_pd.rename(columns={\"B01003_001E\": \"Population\",\n",
    "                                      \"B19013_001E\": \"Median Household Income\",\n",
    "                                      \"B17001_002E\": \"Poverty count\",\n",
    "                                      \"B08301_001E\": \"Commuting count\",\n",
    "                                      \"B08301_003E\": \"Commuting by car count\",\n",
    "                                      \"B08101_041E\": \"Commuting OTHER count\",\n",
    "                                      \"NAME\": \"Name\", \"tract\": \"Census Tract\"})\n",
    "\n",
    "# Add in Poverty Rate (Poverty Count / Population)\n",
    "b_census_pd[\"Poverty Rate\"] = 100 * \\\n",
    "    b_census_pd[\"Poverty count\"].astype(\n",
    "        int) / b_census_pd[\"Population\"].astype(int)\n",
    "\n",
    "# Calculate commute by car)\n",
    "b_census_pd[\"Car Rate\"] = 100 * \\\n",
    "    b_census_pd[\"Commuting by car count\"].astype(\n",
    "        int) / b_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate commute by OTHER\n",
    "b_census_pd[\"Commute OTHER rate\"] = 100 * \\\n",
    "    b_census_pd[\"Commuting OTHER count\"].astype(\n",
    "        int) / b_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate GEOID <= this is used to join to census geography\n",
    "b_census_pd[\"GEOID\"] = b_census_pd[\"state\"].astype(str)+b_census_pd[\"county\"]+b_census_pd[\"Census Tract\"]\n",
    "# b_census_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get census data for Portland\n",
    "# needed to pull several counties because Portland is in 3 different counties\n",
    "p_census_data = c.acs5.get(('NAME','B01003_001E', 'B19013_001E', 'B17001_002E', 'B08301_001E', 'B08301_003E', 'B08101_041E',), geo={'for': 'tract:*',\n",
    "                       'in': 'state:{} county:051,067,005'.format(states.OR.fips)}) # counties 051,067,005 are Multnomah, Washington, Clackamas\n",
    "\n",
    "# Convert to DataFrame\n",
    "p_census_pd = pd.DataFrame(p_census_data)\n",
    "\n",
    "# Column Reordering\n",
    "p_census_pd = p_census_pd.rename(columns={\"B01003_001E\": \"Population\",\n",
    "                                      \"B19013_001E\": \"Median Household Income\",\n",
    "                                      \"B17001_002E\": \"Poverty count\",\n",
    "                                      \"B08301_001E\": \"Commuting count\",\n",
    "                                      \"B08301_003E\": \"Commuting by car count\",\n",
    "                                      \"B08101_041E\": \"Commuting OTHER count\",\n",
    "                                      \"NAME\": \"Name\", \"tract\": \"Census Tract\"})\n",
    "\n",
    "# Add in Poverty Rate (Poverty Count / Population)\n",
    "p_census_pd[\"Poverty Rate\"] = 100 * \\\n",
    "    p_census_pd[\"Poverty count\"].astype(\n",
    "        int) / p_census_pd[\"Population\"].astype(int)\n",
    "\n",
    "# Calculate commute by car)\n",
    "p_census_pd[\"Car Rate\"] = 100 * \\\n",
    "    p_census_pd[\"Commuting by car count\"].astype(\n",
    "        int) / p_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate commute by OTHER\n",
    "p_census_pd[\"Commute OTHER rate\"] = 100 * \\\n",
    "    p_census_pd[\"Commuting OTHER count\"].astype(\n",
    "        int) / p_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate GEOID <= this is used to join to census geography\n",
    "p_census_pd[\"GEOID\"] = p_census_pd[\"state\"].astype(str)+p_census_pd[\"county\"]+p_census_pd[\"Census Tract\"]\n",
    "# p_census_pd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the lowest median household income tracts and highest poverty tracts\n",
    "\n",
    "# <<<<<<< Minneapolis >>>>>>>\n",
    "# Sort by median household income, export low 25 to new df, create new column indicating if it is a low 25 census tract\n",
    "m_census_pd = m_census_pd.sort_values(\"Median Household Income\", ascending=True)\n",
    "m_mhi_25 = m_census_pd.head(25)\n",
    "m_census_pd[\"MHI_25\"] = np.where(m_census_pd['Median Household Income']<=m_mhi_25[\"Median Household Income\"].max(), 'yes', 'no')\n",
    "\n",
    "# Sort by poverty rate, export top 25 to new df, create new column indicating if it is a top 25 tract for poverty rate\n",
    "m_census_pd = m_census_pd.sort_values(\"Poverty Rate\", ascending=False)\n",
    "m_pvt_25 = m_census_pd.head(25)\n",
    "m_census_pd[\"PVT_25\"] = np.where(m_census_pd['Poverty Rate']>=m_pvt_25[\"Poverty Rate\"].min(), 'yes', 'no')\n",
    "\n",
    "# <<<<<<< Boston >>>>>>>\n",
    "# Sort by median household income, export low 25 to new df, create new column indicating if it is a low 25 census tract\n",
    "b_census_pd = b_census_pd.sort_values(\"Median Household Income\", ascending=True)\n",
    "b_mhi_25 = b_census_pd.head(25)\n",
    "b_census_pd[\"MHI_25\"] = np.where(b_census_pd['Median Household Income']<=b_mhi_25[\"Median Household Income\"].max(), 'yes', 'no')\n",
    "\n",
    "# Sort by poverty rate, export top 25 to new df, create new column indicating if it is a top 25 tract for poverty rate\n",
    "b_census_pd = b_census_pd.sort_values(\"Poverty Rate\", ascending=False)\n",
    "b_pvt_25 = b_census_pd.head(25)\n",
    "b_census_pd[\"PVT_25\"] = np.where(b_census_pd['Poverty Rate']>=b_pvt_25[\"Poverty Rate\"].min(), 'yes', 'no')\n",
    "\n",
    "# <<<<<<< Portland >>>>>>>\n",
    "# Portland -  Sort by median household income, export low 25 to new df, create new column indicating if it is a low 25 census tract\n",
    "p_census_pd = p_census_pd.sort_values(\"Median Household Income\", ascending=True)\n",
    "p_mhi_25 = p_census_pd.head(25)\n",
    "p_census_pd[\"MHI_25\"] = np.where(p_census_pd['Median Household Income']<=b_mhi_25[\"Median Household Income\"].max(), 'yes', 'no')\n",
    "\n",
    "# Sort by poverty rate, export top 25 to new df, create new column indicating if it is a top 25 tract for poverty rate\n",
    "p_census_pd = p_census_pd.sort_values(\"Poverty Rate\", ascending=False)\n",
    "p_pvt_25 = p_census_pd.head(25)\n",
    "p_census_pd[\"PVT_25\"] = np.where(p_census_pd['Poverty Rate']>=b_pvt_25[\"Poverty Rate\"].min(), 'yes', 'no')\n",
    "\n",
    "# export the new data frames to csv, to be read in by our geopandas python file\n",
    "m_census_pd.to_csv('census_output/m_census_25.csv')\n",
    "b_census_pd.to_csv('census_output/b_census_25.csv')\n",
    "p_census_pd.to_csv('census_output/p_census_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===========================================\n",
    "# # need to go and run geo_merge_census.py \n",
    "# # this will read in our x_census_25 tables, and join them to a census shapefile for each city\n",
    "# # then it will join bike ride data to census tracts based on start locations\n",
    "# # ==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in csv of spatially joined census and bike data (these are HUGE)\n",
    "m_census_bike = pd.read_csv('census_output/m_sjoin.csv', index_col=0)\n",
    "b_census_bike = pd.read_csv('census_output/b_sjoin.csv', index_col=0)\n",
    "p_census_bike = pd.read_csv('census_output/p_sjoin.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create pie charts to show how many rides are occuring in lowest 25% or median household income\n",
    "\n",
    "# Extract a subset of the data frames containing only the fields that we care about\n",
    "m_census_bike_sub = m_census_bike[[\"GEOID\", \"Population\", \"Median Household Income\", \"Poverty Rate\", \"MHI_25\", \"PVT_25\", \"start station name\"]]\n",
    "b_census_bike_sub = b_census_bike[[\"GEOID\", \"Population\", \"Median Household Income\", \"Poverty Rate\", \"MHI_25\", \"PVT_25\", \"start station name\"]]\n",
    "# need to remove rows with missing data\n",
    "b_census_bike_sub = b_census_bike_sub[b_census_bike_sub[\"Median Household Income\"] != -666666666]\n",
    "p_census_bike_sub = p_census_bike[[\"GEOID\", \"Population\", \"Median Household Income\", \"Poverty Rate\", \"MHI_25\", \"PVT_25\", \"StartHub\"]]\n",
    "p_census_bike_sub = p_census_bike_sub[p_census_bike_sub[\"Median Household Income\"] != -666666666]\n",
    "\n",
    "# Create a function to create pie charts for MHI\n",
    "def piechart_mhi(df_subset, city):\n",
    "    # Find out how trips are in each group\n",
    "    mhi_groups = df_subset.groupby('MHI_25')\n",
    "    # # Chart our data, give it a title\n",
    "    explode = (.1, 0)\n",
    "    mhi_chart = mhi_groups['MHI_25'].count().plot(kind=\"pie\", title=(f\"{city} bike trips in lowest 25% of Median Household Income\"),\n",
    "                                               autopct=\"%1.1f%%\", explode = explode, startangle=140, shadow=True,)\n",
    "    mhi_chart.set_xlabel(\"\")\n",
    "    mhi_chart.set_ylabel(\"\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Create a function to create pie charts for Poverty Rate\n",
    "def piechart_pvt(df_subset, city):\n",
    "    # Find out how trips are in each group\n",
    "    pvt_groups = df_subset.groupby('PVT_25')\n",
    "    # # Chart our data, give it a title\n",
    "    explode = (.1, 0)\n",
    "    pvt_chart = pvt_groups['PVT_25'].count().plot(kind=\"pie\", title=(f\"{city} bike trips in highest 25% of Poverty Rate\"),\n",
    "                                               autopct=\"%1.1f%%\", colors = ['red', 'purple'], explode = explode, startangle=140, shadow=True,)\n",
    "    pvt_chart.set_xlabel(\"\")\n",
    "    pvt_chart.set_ylabel(\"\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# run functions\n",
    "piechart_mhi(m_census_bike_sub, \"Minneapolis\")\n",
    "piechart_mhi(b_census_bike_sub, \"Boston\")\n",
    "piechart_mhi(p_census_bike_sub, \"Portland\")\n",
    "piechart_pvt(m_census_bike_sub, \"Minneapolis\")\n",
    "piechart_pvt(b_census_bike_sub, \"Boston\")\n",
    "piechart_pvt(p_census_bike_sub, \"Portland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run correlation between bike rides and median household income\n",
    "\n",
    "# Create a function to create pie charts for MHI\n",
    "def linregress(df_subset, city):\n",
    "    # Find out how many trips are in each census tract\n",
    "    geoid_grouped = df_subset.groupby(['GEOID'])\n",
    "    tract_mhi = geoid_grouped['Median Household Income'].mean()\n",
    "    tract_trips = geoid_grouped['Median Household Income'].count()\n",
    "    # perform linear regression and calculate regress values\n",
    "    (slope, intercept, rvalue, pvalue, stderr) = st.linregress(tract_mhi, tract_trips)\n",
    "    regress_values = tract_mhi * slope + intercept\n",
    "    # Create the line equation\n",
    "    line_eq = \"y = \" + str(round(slope,2)) + \"x + \" + str(round(intercept,2))\n",
    "    # plot the x and y columns, annotate the line equation\n",
    "    plt.plot(tract_mhi,regress_values,\"b-\")\n",
    "    plt.annotate(line_eq,(tract_mhi.min() + 5000,tract_trips.max() -5000),fontsize=15,color=\"blue\")\n",
    "    plt.scatter( tract_mhi, tract_trips, marker=\"o\", color=\"red\")\n",
    "    plt.title(f\"{city} census tract Median Household Income vs Number of Trips \")\n",
    "    plt.xlabel(\"Median Household Income\")\n",
    "    plt.ylabel(\"Number of Trips\")\n",
    "    print(f\"The r-squared is: {rvalue}\")\n",
    "    plt.show()\n",
    "\n",
    "linregress(m_census_bike_sub, \"Minneapolis\")\n",
    "linregress(b_census_bike_sub, \"Boston\")\n",
    "linregress(p_census_bike_sub, \"Portland\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test null hypothesis on whehter high income areas ride bike shares more often or not\n",
    "# Extract individual groups\n",
    "m_geoid_grouped = m_census_bike_sub.groupby(['GEOID'])\n",
    "b_geoid_grouped = b_census_bike_sub.groupby(['GEOID'])\n",
    "p_geoid_grouped = p_census_bike_sub.groupby(['GEOID'])\n",
    "\n",
    "\n",
    "# Note: Setting equal_var=False performs Welch's t-test which does \n",
    "# not assume equal population variance\n",
    "def ttest(dataset1, dataset2, name1, name2):\n",
    "    test = st.ttest_ind(dataset1['Median Household Income'].count(), \n",
    "             dataset2['Median Household Income'].count(),\n",
    "             equal_var=False)\n",
    "    return print(f\"{name1} and {name2} dataset pvalue:{round(test[1], 5)}\" )\n",
    "ttest(m_geoid_grouped, b_geoid_grouped, \"Minneapolis\", \"Boston\" )\n",
    "ttest(m_geoid_grouped, p_geoid_grouped, \"Minneapolis\", \"Portland\" )\n",
    "ttest(b_geoid_grouped, p_geoid_grouped, \"Boston\", \"Portland\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# End of Census vs bike (Carl's) code\n",
    "# ==================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paul's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Snow Emergency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICAH'S CODE SECTION \n",
    "\n",
    "# =================================================================================================\n",
    "# IDENTIFYING AND CLEANING OUR INDIVIDUAL CITY BIKE SHARE DATAFRAMES FOR OUR DATES FROM CARL'S CODE\n",
    "# =================================================================================================\n",
    "\n",
    "# PORTLAND 'BIKETOWN' BIKE SHARE INFORMATION DATAFRAME FOR 2018 ----------------------------------\n",
    "# SELECTING DESIRED COLUMNS - MONTH : 'Month', DISTANCE : 'Distance_Miles', USER TYPE : 'PaymentPlan', \n",
    "# TRIP DURATION : 'Duration' AND START DATE : 'StartDate'\n",
    "p_df_desired = p_df[[\"Month\", \"Distance_Miles\", \"PaymentPlan\", \"Duration\", \"StartDate\"]]\n",
    "\n",
    "# CLEANING DATAFRAME COLUMN NAMES FOR PORTLAND BIKE SHARE INFORMATION\n",
    "p_df_clean_m = p_df_desired.rename(columns = {\"Distance_Miles\": \"Distance Traveled (mi)\",\n",
    "                                              \"PaymentPlan\": \"User Type\",\n",
    "                                              \"Duration\" : \"Trip Duration\",\n",
    "                                              \"StartDate\" : \"Trip Date\"})\n",
    "\n",
    "# MINNEAPOLIS 'NICERIDE' BIKE SHARE INFORMATION DATAFRAME FOR 2018 ------------------------------\n",
    "# SELECTING DESIRED COLUMNS - MONTH : 'Month', USER TYPE : 'usertype', GENDER : 'gender', TRIP DURATION : 'tripduration'\n",
    "# AND START DATE : 'start_time'\n",
    "m_df_desired = m_df[[\"Month\", \"usertype\", \"gender\", \"tripduration\", \"start_time\"]]\n",
    "\n",
    "# CLEANING DATAFRAME COLUMN NAMES FOR MINNEAPOLIS BIKE SHARE INFORMATION\n",
    "m_df_clean_m = m_df_desired.rename(columns = {\"usertype\": \"User Type\",\n",
    "                                              \"gender\": \"User Gender\",\n",
    "                                              \"tripduration\" : \"Trip Duration\",\n",
    "                                              \"start_time\" : \"Trip Date\"})\n",
    "\n",
    "# BOSTON 'BLUEBIKES' BIKE SHARE INFORMATION DATAFRAME FOR 2018 ----------------------------------\n",
    "# SELECTING DESIRED COLUMNS - MONTH : 'Month', USER TYPE : 'usertype', GENDER : 'gender', TRIP DURATION : 'tripduration'\n",
    "# AND START DATE : 'starttime'\n",
    "b_df_desired = b_df[[\"Month\", \"usertype\", \"gender\", \"tripduration\", \"starttime\"]]\n",
    "\n",
    "# CLEANING DATAFRAME COLUMN NAMES FOR BOSTON BIKE SHARE INFORMATION\n",
    "b_df_clean_m = b_df_desired.rename(columns = {\"usertype\": \"User Type\",\n",
    "                                              \"gender\": \"User Gender\",\n",
    "                                              \"tripduration\" : \"Trip Duration\",\n",
    "                                              \"starttime\" : \"Trip Date\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_clean_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df_clean_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df_clean_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# GETTING HISTORICAL WEATHER DATA FROM 01 JANUARY 2018 THROUGH 31 DECEMBER 2018 (OBTAINED THROUGH NOAA)\n",
    "# =================================================================================================\n",
    "\n",
    "# READING IN .CSV FILE CALLED 'Weather-Data.csv'\n",
    "weather_data_df = pd.read_csv(\"resources/Weather-Data.csv\")\n",
    "\n",
    "weather_data_df\n",
    "\n",
    "# CLEANING DATAFRAME COLUMN NAMES\n",
    "weather_data_df = weather_data_df.rename( columns = {'AWND' : 'Average Wind Speed',\n",
    "                                                     'NAME' : 'Name',\n",
    "                                                     'DATE' : 'Date',\n",
    "                                                     'MDPR' : 'Multiday Precipitation Total',\n",
    "                                                     'PGTM' : 'Peak Gust Time',\n",
    "                                                     'PRCP' : 'Precipitation',\n",
    "                                                     'PSUN' : 'Daily Percent of Possible Sunshine',\n",
    "                                                     'SNOW' : 'Snowfall',\n",
    "                                                     'SNWD' : 'Snow Depth',\n",
    "                                                     'TAVG' : 'Average Temperature',\n",
    "                                                     'TMAX' : 'Maximum Temperature',\n",
    "                                                     'TMIN' : 'Minimum Temperature',\n",
    "                                                     'TOBS' : 'Temperature at Time of Observation',\n",
    "                                                     'TSUN' : 'Total Sunshine',\n",
    "                                                     'WDMV' : 'Total Wind Movement',\n",
    "                                                     'WT01' : 'Fog, Ice Fog, or Freezing Fog',\n",
    "                                                     'WT02' : 'Heavy Fog or Heaving Freezing Fog',\n",
    "                                                     'WT03' : 'Thunder',\n",
    "                                                     'WT04' : 'Ice pellets, Sleet, Snow Pellets, or Small Hail',\n",
    "                                                     'WT05' : 'Hail',\n",
    "                                                     'WT08' : 'Smoke or Haze',\n",
    "                                                     'WT09' : 'Blowing or Drifting Snow',\n",
    "                                                     'WT11' : 'High or Damaging Winds'})\n",
    "\n",
    "# FILLING NAN VALUES WITH ZEROS\n",
    "weather_data_df_cleaned = weather_data_df.fillna(0)\n",
    "\n",
    "# SELECTING ONLY COLUMNS WE WANT AND HAVE CLEANED\n",
    "weather_data_df_cleansed = weather_data_df_cleaned[[\"Name\",\n",
    "                                                    \"Date\",\n",
    "                                                    \"Average Wind Speed\",\n",
    "                                                    \"Multiday Precipitation Total\",\n",
    "                                                    \"Peak Gust Time\",\n",
    "                                                    \"Precipitation\",\n",
    "                                                    \"Daily Percent of Possible Sunshine\",\n",
    "                                                    \"Snowfall\",\n",
    "                                                    \"Snow Depth\",\n",
    "                                                    \"Average Temperature\",\n",
    "                                                    \"Maximum Temperature\",\n",
    "                                                    \"Minimum Temperature\",\n",
    "                                                    \"Temperature at Time of Observation\",\n",
    "                                                    \"Total Sunshine\",\n",
    "                                                    \"Total Wind Movement\",\n",
    "                                                    \"Fog, Ice Fog, or Freezing Fog\",\n",
    "                                                    \"Heavy Fog or Heaving Freezing Fog\",\n",
    "                                                    \"Thunder\",\n",
    "                                                    \"Ice pellets, Sleet, Snow Pellets, or Small Hail\",\n",
    "                                                    \"Hail\",\n",
    "                                                    \"Smoke or Haze\",\n",
    "                                                    \"Blowing or Drifting Snow\",\n",
    "                                                    \"High or Damaging Winds\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_df_cleansed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
