{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bikeshare Comparisons Across Three Cities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in ridership across three major cities throughout the country (East, Midwest, West) during 2018. We will uncover relationships between users and their: age, gender, etc. We will also look at the relationships between ridership and weather using the US weather data API as well as using the US Census API to investigate relationships between median household income of geographic areas and volume of rides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* East Coast: Boston, MA\n",
    "    * Data source: [Blue Bikes](https://www.bluebikes.com/system-data)\n",
    "* Midwest: Minneapolis, MN\n",
    "    * Data source: [niceride](https://www.niceridemn.com/system-data)\n",
    "* West Coast: Portland, OR\n",
    "    * Data source: [BIKETOWN](https://www.biketownpdx.com/system-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/bike-share.jpg \"Niceride\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bikeshare company of each city openly makes their data available in one month intervals. These can be downloaded as CSV files. Some data fields include:\n",
    "* Origin station name and coordinates\n",
    "* Desitnation station name and coordinates\n",
    "* Date and time\n",
    "* Rider age and gender\n",
    "\n",
    "We also looked at [BikeShare-Research.org](https://bikeshare-research.org/). This is an open API with bikeshare data around the world. We decided not to use this resoruce because the data was only broken down by number of rides per day in each city. We wanted to pursue an option where we could look at individual trips and riders. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in our mods\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "# import census\n",
    "from census import Census\n",
    "from us import states\n",
    "# Census API Key\n",
    "from config import api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the bike data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to read in the data from each city. Because they came in individual months and we wanted data for the entire year, we needed to add a field for month and then append each months data to a new data frame. Below is the example for Minneapolis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the individual bike files, append a new column for month\n",
    "m_df1 = pd.read_csv(\"resources/Minneapolis/201804-niceride-tripdata.csv\")\n",
    "m_df1[\"Month\"] = \"April\"\n",
    "m_df2 = pd.read_csv(\"resources/Minneapolis/201805-niceride-tripdata.csv\")\n",
    "m_df2[\"Month\"] = \"May\"\n",
    "m_df3 = pd.read_csv(\"resources/Minneapolis/201806-niceride-tripdata.csv\")\n",
    "m_df3[\"Month\"] = \"June\"\n",
    "m_df4 = pd.read_csv(\"resources/Minneapolis/201807-niceride-tripdata.csv\")\n",
    "m_df4[\"Month\"] = \"July\"\n",
    "m_df5 = pd.read_csv(\"resources/Minneapolis/201808-niceride-tripdata.csv\")\n",
    "m_df5[\"Month\"] = \"August\"\n",
    "m_df6 = pd.read_csv(\"resources/Minneapolis/201809-niceride-tripdata.csv\")\n",
    "m_df6[\"Month\"] = \"September\"\n",
    "m_df7 = pd.read_csv(\"resources/Minneapolis/201810-niceride-tripdata.csv\")\n",
    "m_df7[\"Month\"] = \"October\"\n",
    "m_df8 = pd.read_csv(\"resources/Minneapolis/201811-niceride-tripdata.csv\")\n",
    "m_df8[\"Month\"] = \"November\"\n",
    "\n",
    "# create a new data frame and append individual data frames together\n",
    "m_df = pd.DataFrame()\n",
    "m_df = m_df1.append(m_df2).append(m_df3).append(m_df4).append(m_df5).append(m_df6).append(m_df7).append(m_df8)\n",
    "?m_df.to_csv('census_output/m_bike.csv')\n",
    "m_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering census and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our goals with this study was to determine if median household income has any relationship to the number of rides. \n",
    "\n",
    "To do this, we needed to gather some demographic data in a geography that can be spatially related to bike share trips. Census tracts are a common geography to map demographic data. Therefore, we used the US Census API to gather Median Household Income (referred to as MHI below), Poverty Rate, and percentage of people who drive to work per census tract.\n",
    "\n",
    "We could not determine how to extract only the census tracts within the boundaries of each city, so instead we extracted the census tracts for the county(ies) in which each city resides (below, we filtered this down to specifically the boundaries of each city using shapefiles).\n",
    "\n",
    "Below is an example of the census tract extraction for Minneapolis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get census data for Minneapolis\n",
    "c = Census(api_key, year=2017)\n",
    "m_census_data = c.acs5.get(('NAME','B01003_001E', 'B19013_001E', 'B17001_002E', 'B08301_001E', 'B08301_003E', 'B08101_041E',), geo={'for': 'tract:*',\n",
    "                       'in': 'state:{} county:053'.format(states.MN.fips)}) #  county:053 is Hennepin County\n",
    "# Convert to DataFrame\n",
    "m_census_pd = pd.DataFrame(m_census_data)\n",
    "\n",
    "# Column Renaming\n",
    "m_census_pd = m_census_pd.rename(columns={\"B01003_001E\": \"Population\",\n",
    "                                      \"B19013_001E\": \"Median Household Income\",\n",
    "                                      \"B17001_002E\": \"Poverty count\",\n",
    "                                      \"B08301_001E\": \"Commuting count\",\n",
    "                                      \"B08301_003E\": \"Commuting by car count\",\n",
    "                                      \"B08101_041E\": \"Commuting OTHER count\",\n",
    "                                      \"NAME\": \"Name\", \"tract\": \"Census Tract\"})\n",
    "\n",
    "# Add in Poverty Rate (Poverty Count / Population)\n",
    "m_census_pd[\"Poverty Rate\"] = 100 * \\\n",
    "    m_census_pd[\"Poverty count\"].astype(\n",
    "        int) / m_census_pd[\"Population\"].astype(int)\n",
    "\n",
    "# Calculate commute by car\n",
    "m_census_pd[\"Car Rate\"] = 100 * \\\n",
    "    m_census_pd[\"Commuting by car count\"].astype(\n",
    "        int) / m_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate commute by OTHER\n",
    "m_census_pd[\"Commute OTHER rate\"] = 100 * \\\n",
    "    m_census_pd[\"Commuting OTHER count\"].astype(\n",
    "        int) / m_census_pd[\"Commuting count\"].astype(int)\n",
    "\n",
    "# Calculate GEOID <= this is used to join to census geography below\n",
    "m_census_pd[\"GEOID\"] = m_census_pd[\"state\"].astype(str)+m_census_pd[\"county\"]+m_census_pd[\"Census Tract\"]\n",
    "m_census_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these bikeshare companies providing their services in disadvantaged neighborhoods? Is everyone granted the same access to these healthy transportation alternatives? \n",
    "\n",
    "One route we can take to determine this is to see how the census tracts with the lowest 25% of Median Household Income perform. Additionally, we can see how the census tracts with the highest poverty rates perform. Below, we add columns to the end of our census data that tells us if that tract is in the lowest 25% of MHI or highest 25% of poverty rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the lowest median household income tracts and highest poverty tracts\n",
    "\n",
    "# <<<<<<< Minneapolis >>>>>>>\n",
    "# Sort by median household income, export low 25 to new df, create new column indicating if it is a low 25 census tract\n",
    "m_census_pd = m_census_pd.sort_values(\"Median Household Income\", ascending=True)\n",
    "m_mhi_25 = m_census_pd.head(25)\n",
    "m_census_pd[\"MHI_25\"] = np.where(m_census_pd['Median Household Income']<=m_mhi_25[\"Median Household Income\"].max(), 'yes', 'no')\n",
    "\n",
    "# Sort by poverty rate, export top 25 to new df, create new column indicating if it is a top 25 tract for poverty rate\n",
    "m_census_pd = m_census_pd.sort_values(\"Poverty Rate\", ascending=False)\n",
    "m_pvt_25 = m_census_pd.head(25)\n",
    "m_census_pd[\"PVT_25\"] = np.where(m_census_pd['Poverty Rate']>=m_pvt_25[\"Poverty Rate\"].min(), 'yes', 'no')\n",
    "m_census_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to determine how many rides originated in each census tract. \n",
    "\n",
    "We can't perform a merge, because there are no common columns between our census data and our bike data. \n",
    "\n",
    "However, our bike data has coordinates for the origin of each bike trip. We'll need to make use of those coordinates to determine how many are found in each census tract. Although the demographic data that we extracted from the US Census API did not include any spatial information (geometry), we can find geometry for these census tracts from other soruces. \n",
    "\n",
    "We downloaded shapefiles of census tracts for Massachusetts, Minnesota and Oregon from the US Censu Bureau.  Due to some limitations in how the US Census Bureau provides their data, we could not find a straighforward way to filter down to only the census tracts in each city, so some GIS software was used to filter these shapefiles down to just the boundaries of Boston, Minneapolis and Portland. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are now working with data that has spatial geometry, we needed to use a new library: [GeoPandas](http://geopandas.org/)\n",
    "\n",
    "We initially had issues running geopandas in jupyter lab/notebook, and we also only needed one of us to download/learn/use geopandas. Therefore, we created a standalong python file to perform the geopandas work. \n",
    "\n",
    "This .py file uses geopandas to import the filtered shapefiles. Then, using the GEOID that we generated in the Census API data above, we joined our demographic data to these census shapefiles. Now our census data is related to a spatial location in a \"geo data frame\"\n",
    "\n",
    "Then we use the coordinate fields of the bike data to create a geo data frame. \n",
    "\n",
    "The final step in this .py file is to perform a spatial join and join each bike trip to a census tract. Therefore, we can produce demographic data for each bike trip. See the geopandas code below (don't run, it takes forever). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<THIS LINE IS INTENDED TO BREAK THIS CODE BLOCK, WE DON'T WANT TO RUN IT>\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# # Read in the shapefiles of city census tracts ( with their geometry)\n",
    "m_census_gdf = gpd.read_file(\"Census_subsets/Mpls_CensusTracts.shp\")\n",
    "# # convert the GEOID field to an integer for merging later\n",
    "m_census_gdf = m_census_gdf.astype({'GEOID': 'int64'})\n",
    "b_census_gdf = gpd.read_file(\"Census_subsets/Bos_CensusTracts.shp\")\n",
    "b_census_gdf = b_census_gdf.astype({'GEOID': 'int64'})\n",
    "p_census_gdf = gpd.read_file(\"Census_subsets/Por_CensusTracts.shp\")\n",
    "p_census_gdf = p_census_gdf.astype({'GEOID': 'int64'})\n",
    "\n",
    "# # Read in the ACS census data from the census api\n",
    "m_census_df = pd.read_csv('census_output/m_census_25.csv', index_col=0)\n",
    "b_census_df = pd.read_csv('census_output/b_census_25.csv', index_col=0)\n",
    "p_census_df = pd.read_csv('census_output/p_census_25.csv', index_col=0)\n",
    "\n",
    "# # Merge the census api data to the census geometry data adn export to csv\n",
    "m_merged = m_census_gdf.merge(m_census_df, on='GEOID')\n",
    "# m_merged.to_csv('census_output/m_merged.csv')\n",
    "b_merged = b_census_gdf.merge(b_census_df, on='GEOID')\n",
    "# b_merged.to_csv('census_output/b_merged.csv')\n",
    "p_merged = p_census_gdf.merge(p_census_df, on='GEOID')\n",
    "# p_merged.to_csv('census_output/p_merged.csv')\n",
    "\n",
    "# # Read in the bike trips info\n",
    "m_bike_df = pd.read_csv('census_output/m_bike.csv', index_col=0)\n",
    "m_bike_df = m_bike_df.rename(columns={\"start station longitude\":\"s_longitude\", \"start station latitude\":\"s_latitude\",\n",
    "    \"end station longitude\":\"e_longitude\", \"end station latitude\":\"e_latitude\"})\n",
    "b_bike_df = pd.read_csv('census_output/b_bike.csv', index_col=0)\n",
    "b_bike_df = b_bike_df.rename(columns={\"start station longitude\":\"s_longitude\", \"start station latitude\":\"s_latitude\",\n",
    "    \"end station longitude\":\"e_longitude\", \"end station latitude\":\"e_latitude\"})\n",
    "p_bike_df = pd.read_csv('census_output/p_bike.csv', index_col=0)\n",
    "p_bike_df = p_bike_df.rename(columns={\"StartLongitude\":\"s_longitude\", \"StartLatitude\":\"s_latitude\",\n",
    "    \"EndLongitude\":\"e_longitude\", \"EndLatitude\":\"e_latitude\"})\n",
    "\n",
    "\n",
    "# Create geo dfs from the city bike data\n",
    "m_bike_gdf = gpd.GeoDataFrame(\n",
    "    m_bike_df, geometry=gpd.points_from_xy(m_bike_df.s_longitude, m_bike_df.s_latitude))\n",
    "b_bike_gdf = gpd.GeoDataFrame(\n",
    "    b_bike_df, geometry=gpd.points_from_xy(b_bike_df.s_longitude, b_bike_df.s_latitude))\n",
    "p_bike_gdf = gpd.GeoDataFrame(\n",
    "    p_bike_df, geometry=gpd.points_from_xy(p_bike_df.s_longitude, p_bike_df.s_latitude))\n",
    "print(\"------------------------------------\")\n",
    "print(\" Geo dfs have been created from the bike data\")\n",
    "#\n",
    "# Warning---- takes a long time to run!!!!!\n",
    "# Spatial join\n",
    "m_sjoin = gpd.sjoin(m_merged, m_bike_gdf, how=\"inner\", op='intersects')\n",
    "m_sjoin.to_csv('census_output/m_sjoin.csv')\n",
    "print(\"------------------------------------\")\n",
    "print(\"Minneapolis data has been joined\")\n",
    "b_sjoin = gpd.sjoin(b_merged, b_bike_gdf, how=\"inner\", op='intersects')\n",
    "b_sjoin.to_csv('census_output/b_sjoin.csv')\n",
    "print(\"------------------------------------\")\n",
    "print(\"Boston data has been joined\")\n",
    "p_sjoin = gpd.sjoin(p_merged, p_bike_gdf, how=\"inner\", op='intersects')\n",
    "p_sjoin.to_csv('census_output/p_sjoin.csv')\n",
    "print(\"------------------------------------\")\n",
    "print(\"Portland data has been joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<THIS LINE IS MEANT TO BREAK THIS CODE BLOCK, TAKES TOO LONG TO RUN>\n",
    "m_census_bike = pd.read_csv('output/m_sjoin.csv', index_col=0)\n",
    "m_census_bike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Images/Mpls_Map.png \"Minneapolis Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the MHI and poverty rate of the census tract in which each bike trip originates. \n",
    "\n",
    "First we'll remove the fields that we don't care about.\n",
    "\n",
    "Then, lets see if the number of trips in the lowest MHI 25% of census tracts and highest poverty rate census tracts are proportional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "< THIS LINE MEANT TO BREAK THIS CODE BLOCK>\n",
    "# Extract a subset of the data frames containing only the fields that we care about\n",
    "m_census_bike_sub = m_census_bike[[\"GEOID\", \"Population\", \"Median Household Income\", \"Poverty Rate\", \"MHI_25\", \"PVT_25\", \"start station name\"]]\n",
    "\n",
    "# Create a function to create pie charts for MHI\n",
    "def piechart_mhi(df_subset, city):\n",
    "    # Find out how trips are in each group\n",
    "    mhi_groups = df_subset.groupby('MHI_25')\n",
    "    # # Chart our data, give it a title\n",
    "    explode = (.1, 0)\n",
    "    mhi_chart = mhi_groups['MHI_25'].count().plot(kind=\"pie\", title=(f\"{city} bike trips in lowest 25% of Median Household Income\"),\n",
    "                                               autopct=\"%1.1f%%\", explode = explode, startangle=140, shadow=True,)\n",
    "    mhi_chart.set_xlabel(\"\")\n",
    "    mhi_chart.set_ylabel(\"\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Create a function to create pie charts for Poverty Rate\n",
    "def piechart_pvt(df_subset, city):\n",
    "    # Find out how trips are in each group\n",
    "    pvt_groups = df_subset.groupby('PVT_25')\n",
    "    # # Chart our data, give it a title\n",
    "    explode = (.1, 0)\n",
    "    pvt_chart = pvt_groups['PVT_25'].count().plot(kind=\"pie\", title=(f\"{city} bike trips in highest 25% of Poverty Rate\"),\n",
    "                                               autopct=\"%1.1f%%\", colors = ['red', 'purple'], explode = explode, startangle=140, shadow=True,)\n",
    "    pvt_chart.set_xlabel(\"\")\n",
    "    pvt_chart.set_ylabel(\"\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# run functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Images/Boston_PVT.png \"Boston pvt\")\n",
    "![alt text](Images/Portland_PVT.png \"Portland PVT\")\n",
    "![alt text](Images/Minneapolis_PVT.png \"Minneapolis PVT\")\n",
    "![alt text](Images/Boston_MHI.png \"Boston mhi\")\n",
    "![alt text](Images/Portland_MHI.png \"Portland mhi\")\n",
    "![alt text](Images/Minneapolis_MHI.png \"Minneapolis MHI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform linear regression on bike data vs MHI.\n",
    "\n",
    "* Boston MHI vs bike trips is: 0.28845154131829437\n",
    "* Portland MHI vs bike trips is: -0.43017099537257414\n",
    "* Minneapolis MHI vs bike trips is: -0.010342502738792694\n",
    "\n",
    "\n",
    "![alt text](Images/Boston_linregress.png \"Boston linregress\")\n",
    "![alt text](Images/Portland_linregress.png \"Portland linregress\")\n",
    "![alt text](Images/Minneapolis_linregress.png \"Minneapolis linregress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct independent TTest: \n",
    "\n",
    "![alt text](Images/Trips_vs_MHI_scatter.png \"Scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "< THIS LINE MEANT TO BREAK THIS CODE BLOCK> \n",
    "# Extract individual groups\n",
    "m_geoid_grouped = m_census_bike_sub.groupby(['GEOID'])\n",
    "b_geoid_grouped = b_census_bike_sub.groupby(['GEOID'])\n",
    "p_geoid_grouped = p_census_bike_sub.groupby(['GEOID'])\n",
    "\n",
    "\n",
    "# Note: Setting equal_var=False performs Welch's t-test which does \n",
    "# not assume equal population variance\n",
    "def ttest(dataset1, dataset2, name1, name2):\n",
    "    test = st.ttest_ind(dataset1['GEOID'].count(), \n",
    "             dataset2['GEOID'].count(),\n",
    "             equal_var=False)\n",
    "    return print(f\"{name1} and {name2} number of trips dataset pvalue:{round(test[1], 5)}\" )\n",
    "ttest(m_geoid_grouped, b_geoid_grouped, \"Minneapolis\", \"Boston\" )\n",
    "ttest(m_geoid_grouped, p_geoid_grouped, \"Minneapolis\", \"Portland\" )\n",
    "ttest(b_geoid_grouped, p_geoid_grouped, \"Boston\", \"Portland\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Minneapolis and Boston number of trips dataset pvalue: 0.02204\n",
    "* Minneapolis and Portland number of trips dataset pvalue: 0.08612\n",
    "* Boston and Portland number of trips dataset pvalue: 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of Paul's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Paul's Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of Micah's Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning for Bike Sharing System Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINNEAPOLIS 'NICERIDE' BIKE SHARE INFORMATION DATAFRAME FOR 2018 ------------------------------\n",
    "# SELECTING DESIRED COLUMNS - MONTH : 'Month', USER TYPE : 'usertype', GENDER : 'gender', TRIP DURATION : 'tripduration'\n",
    "# AND START DATE : 'start_time'\n",
    "m_df_desired = m_df[[\"Month\", \"usertype\", \"gender\", \"tripduration\", \"start_time\"]]\n",
    "\n",
    "# CLEANING DATAFRAME COLUMN NAMES FOR MINNEAPOLIS BIKE SHARE INFORMATION\n",
    "m_df_clean_m = m_df_desired.rename(columns = {\"usertype\": \"User Type\",\n",
    "                                              \"gender\": \"User Gender\",\n",
    "                                              \"tripduration\" : \"Trip Duration\",\n",
    "                                              \"start_time\" : \"Trip Date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning for Weather Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# GETTING HISTORICAL WEATHER DATA FROM 01 JANUARY 2018 THROUGH 31 DECEMBER 2018 (OBTAINED THROUGH NOAA)\n",
    "# =================================================================================================\n",
    "\n",
    "# READING IN .CSV FILE CALLED 'Weather-Data.csv'\n",
    "weather_data_df = pd.read_csv(\"resources/Weather-Data.csv\")\n",
    "\n",
    "weather_data_df\n",
    "\n",
    "# CLEANING DATAFRAME COLUMN NAMES\n",
    "weather_data_df = weather_data_df.rename( columns = {'AWND' : 'Average Wind Speed',\n",
    "                                                     'NAME' : 'Name',\n",
    "                                                     'DATE' : 'Date',\n",
    "                                                     'MDPR' : 'Multiday Precipitation Total',\n",
    "                                                     'PGTM' : 'Peak Gust Time',\n",
    "                                                     'PRCP' : 'Precipitation',\n",
    "                                                     'PSUN' : 'Daily Percent of Possible Sunshine',\n",
    "                                                     'SNOW' : 'Snowfall',\n",
    "                                                     'SNWD' : 'Snow Depth',\n",
    "                                                     'TAVG' : 'Average Temperature',\n",
    "                                                     'TMAX' : 'Maximum Temperature',\n",
    "                                                     'TMIN' : 'Minimum Temperature',\n",
    "                                                     'TOBS' : 'Temperature at Time of Observation',\n",
    "                                                     'TSUN' : 'Total Sunshine',\n",
    "                                                     'WDMV' : 'Total Wind Movement',\n",
    "                                                     'WT01' : 'Fog, Ice Fog, or Freezing Fog',\n",
    "                                                     'WT02' : 'Heavy Fog or Heaving Freezing Fog',\n",
    "                                                     'WT03' : 'Thunder',\n",
    "                                                     'WT04' : 'Ice pellets, Sleet, Snow Pellets, or Small Hail',\n",
    "                                                     'WT05' : 'Hail',\n",
    "                                                     'WT08' : 'Smoke or Haze',\n",
    "                                                     'WT09' : 'Blowing or Drifting Snow',\n",
    "                                                     'WT11' : 'High or Damaging Winds'})\n",
    "\n",
    "# FILLING NAN VALUES WITH ZEROS\n",
    "weather_data_df_cleaned = weather_data_df.fillna(0)\n",
    "\n",
    "# SELECTING ONLY COLUMNS WE WANT AND HAVE CLEANED\n",
    "weather_data_df_cleansed = weather_data_df_cleaned[[\"Name\",\n",
    "                                                    \"Date\",\n",
    "                                                    \"Average Wind Speed\",\n",
    "                                                    \"Multiday Precipitation Total\",\n",
    "                                                    \"Peak Gust Time\",\n",
    "                                                    \"Precipitation\",\n",
    "                                                    \"Daily Percent of Possible Sunshine\",\n",
    "                                                    \"Snowfall\",\n",
    "                                                    \"Snow Depth\",\n",
    "                                                    \"Average Temperature\",\n",
    "                                                    \"Maximum Temperature\",\n",
    "                                                    \"Minimum Temperature\",\n",
    "                                                    \"Temperature at Time of Observation\",\n",
    "                                                    \"Total Sunshine\",\n",
    "                                                    \"Total Wind Movement\",\n",
    "                                                    \"Fog, Ice Fog, or Freezing Fog\",\n",
    "                                                    \"Heavy Fog or Heaving Freezing Fog\",\n",
    "                                                    \"Thunder\",\n",
    "                                                    \"Ice pellets, Sleet, Snow Pellets, or Small Hail\",\n",
    "                                                    \"Hail\",\n",
    "                                                    \"Smoke or Haze\",\n",
    "                                                    \"Blowing or Drifting Snow\",\n",
    "                                                    \"High or Damaging Winds\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING 'Name' COLUMN TO FIRST WORD ONLY AND PLACING IN NEW COLUMN CALLED 'city_name'\n",
    "weather_data_df_cleansed[\"city_name\"] = weather_data_df_cleansed[\"Name\"].str.split(\" \").str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Do subscribers or daily customers mostly support the bike share systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOKING AT MINNEAPOLIS USER TYPE DATA\n",
    "\n",
    "# GET DISTANCE BETWEEN LATITUDE AND LONGITUDE POINTS FOR MINNEAPOLIS RIDES\n",
    "from geopy import distance\n",
    "\n",
    "m_start_lat = list(m_df[\"start station latitude\"])\n",
    "m_start_lng = list(m_df[\"start station longitude\"])\n",
    "m_end_lat = list(m_df[\"end station latitude\"])\n",
    "m_end_lng = list(m_df[\"end station longitude\"])\n",
    "\n",
    "i = 0 \n",
    "\n",
    "minneapolis_trip_distances = []\n",
    "\n",
    "for i in np.arange(len(m_start_lat)):\n",
    "    \n",
    "    minneapolis_trip_distances.append(distance.distance((m_start_lat[i], m_start_lng[i]), (m_end_lat[i], m_end_lng[i])).miles)\n",
    "   \n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TRIP DISTANCES COLUMN TO THE MINNEAPOLIS BIKE SHARE DATAFRAME\n",
    "m_df_clean_m[\"Distance Traveled (mi)\"] = minneapolis_trip_distances\n",
    "\n",
    "# AVERAGE DISTRANCE TRAVELED FOR EACH GROUP OF USER CLASSIFICATIONS IN MINEAPOLIS FOR ENTIRE 2018\n",
    "average_annual_distance_by_user_type_m = m_df_clean_m.groupby([\"User Type\"])[\"Distance Traveled (mi)\"].mean()\n",
    "\n",
    "# TOTAL DISTANCE TRAVLED BY USER TYPES OVER THE YEAR IN PORTLAND\n",
    "total_distance_by_user_type_m = m_df_clean_m.groupby([\"User Type\"])[\"Distance Traveled (mi)\"].sum()\n",
    "\n",
    "# COUNT OF HOW MANY RIDES WERE TAKEN FOR USER TYPES THERE ARE IN PORTLAND\n",
    "rides_per_user_type_m = m_df_clean_m[\"User Type\"].value_counts()\n",
    "\n",
    "user_type_list_m = list(m_df_clean_m[\"User Type\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING MINNEAPOLIS USER TYPE COMPARISONS\n",
    "# PLOTTING RIDE COUNT PER USER TYPE IN MINNEAPOLIS\n",
    "plt.bar(user_type_list_m, rides_per_user_type_m, color = 'black')\n",
    "plt.title('Number of Rides Taken in Minneapolis, MN by User Type in 2018')\n",
    "plt.xlabel('User Type')\n",
    "plt.ylabel('Number of Rides')\n",
    "plt.savefig(\"Images/number_rides_user_minneapolis.png\")\n",
    "plt.show()\n",
    "\n",
    "# plotting subscriber type vs mean distance traveled per trip\n",
    "plt.bar(user_type_list_m, average_annual_distance_by_user_type_m, color = 'black')\n",
    "plt.title('Average Trip Distance Traveled by User Type in Minneapolis, MN during 2018')\n",
    "plt.xlabel('User Type')\n",
    "plt.ylabel('Distance Traveled (mi)')\n",
    "plt.savefig(\"Images/mean_distance_per_user_minneapolis.png\")\n",
    "plt.show()\n",
    "\n",
    "# # plotting subscriber type vs total distance traveled \n",
    "plt.bar(user_type_list_m, total_distance_by_user_type_m, color = 'black')\n",
    "plt.title('Total Distance Traveled by User Type in Minneapolis, MN during 2018')\n",
    "plt.xlabel('User Type')\n",
    "plt.ylabel('Distance Traveled (mi)')\n",
    "plt.savefig(\"Images/total_distance_per_user_minneapolis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for all three cities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/mean_distance_per_user_portland.png)\n",
    "![title](Images/mean_distance_per_user_minneapolis.png)\n",
    "![title](Images/mean_distance_per_user_boston.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/number_rides_user_boston.png)\n",
    "![title](Images/number_rides_user_minneapolis.png)\n",
    "![title](Images/number_rides_user_portland.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/total_distance_per_user_boston.png)\n",
    "![title](Images/total_distance_per_user_minneapolis.png)\n",
    "![title](Images/total_distance_per_user_portland.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: The month of the year (warmer temperatures) affects ride volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARING RIDES BY MONTH ===============================================\n",
    "\n",
    "# DEFINING ACTIVE MONTHS LIST FOR EACH CITY\n",
    "active_months_mpls = list(m_df_clean_m[\"Month\"].unique())\n",
    "active_months_boston = list(b_df_clean_m[\"Month\"].unique())\n",
    "active_months_portland = list(p_df_clean_m[\"Month\"].unique())\n",
    "\n",
    "# DEFINING RIDES PER MONTH\n",
    "monthly_rides_m = m_df_clean_m.groupby(\"Month\", sort = False)[\"User Type\"].count()\n",
    "monthly_rides_b = b_df_clean_m.groupby(\"Month\", sort = False)[\"User Type\"].count()\n",
    "monthly_rides_p = p_df_clean_m.groupby(\"Month\", sort = False)[\"User Type\"].count()\n",
    "\n",
    "# PLOTTING RIDES PER MONTH FOR EACH CITY\n",
    "# MINNEAPOLIS RIDES PER MONTH BAR GRAPH\n",
    "plt.bar(active_months_mpls, monthly_rides_m, color = 'black')\n",
    "plt.title('Number of Rides Taken per Month in Minneapolis, MN in 2018')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Rides')\n",
    "plt.savefig(\"Images/number_rides_per_month_mpls.png\")\n",
    "plt.show()\n",
    "\n",
    "# BOSTON RIDES PER MONTH BAR GRAPH\n",
    "plt.bar(active_months_boston, monthly_rides_b, color = 'black')\n",
    "plt.title('Number of Rides Taken per Month in Boston, MA in 2018')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Rides')\n",
    "plt.savefig(\"Images/number_rides_per_month_boston.png\")\n",
    "plt.show()\n",
    "\n",
    "# PORTLAND RIDES PER MONTH BAR GRAPH\n",
    "plt.bar(active_months_portland, monthly_rides_p, color = 'black')\n",
    "plt.title('Number of Rides Taken per Month in Portland, OR in 2018')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Rides')\n",
    "plt.savefig(\"Images/number_rides_per_month_portland.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for Rides per Month: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/number_rides_per_month_boston.png)\n",
    "![title](Images/number_rides_per_month_portland.png)\n",
    "![title](Images/number_rides_per_month_mpls.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Testing on Monthly Ride Volume: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING EACH RIDES BY MONTH SET FOR NORMAL DISTRIBUTION USING scipy.stats.normaltest() ======================================\n",
    "import scipy.stats as sts\n",
    "\n",
    "# minneapolis normal test\n",
    "m_normal_test = sts.normaltest(monthly_rides_m)\n",
    "\n",
    "# boston normal test\n",
    "b_normal_test = sts.normaltest(monthly_rides_b)\n",
    "\n",
    "# portland normal test\n",
    "p_normal_test = sts.normaltest(monthly_rides_p)\n",
    "\n",
    "# print statement sumamrizing normal tests for rides per month across the three cities\n",
    "print(f\"\"\"The p-value for Minneapolis' rides per month data is: {m_normal_test[1]}.\n",
    "The p-value for Boston's rides per month data is: {b_normal_test[1]}.\n",
    "The p-value for Portland's rides per month data is: {p_normal_test[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for ride volume in awful weather: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for ridership in rain\n",
    "portland_rain = portland_merged.loc[(portland_merged[\"Precipitation\"] > 0) & ((portland_merged['User Type'] == 'Subscriber') | (portland_merged['User Type'] == 'Casual')), :]\n",
    "minneapolis_rain = minneapolis_merged.loc[(minneapolis_merged[\"Precipitation\"] > 0) & ((minneapolis_merged['User Type'] == 'Subscriber') | (minneapolis_merged['User Type'] == 'Customer')), :]\n",
    "boston_rain = boston_merged.loc[(boston_merged[\"Precipitation\"] > 0) & ((boston_merged['User Type'] == 'Subscriber') | (boston_merged['User Type'] == 'Customer')), :]\n",
    "\n",
    "print(f\"Number of riders who rode in the rain in Portland: {len(portland_rain)}, in Minneapolis: {len(minneapolis_rain)}, and in Boston: {len(boston_rain)}\")\n",
    "\n",
    "# check for ridership in high winds\n",
    "portland_wind = portland_merged.loc[(portland_merged[\"High or Damaging Winds\"] > 0) & ((portland_merged['User Type'] == 'Subscriber') | (portland_merged['User Type'] == 'Casual')), :]\n",
    "minneapolis_wind = minneapolis_merged.loc[(minneapolis_merged[\"High or Damaging Winds\"] > 0) & ((minneapolis_merged['User Type'] == 'Subscriber') | (minneapolis_merged['User Type'] == 'Customer')), :]\n",
    "boston_wind = boston_merged.loc[(boston_merged[\"High or Damaging Winds\"] > 0) & ((boston_merged['User Type'] == 'Subscriber') | (boston_merged['User Type'] == 'Customer')), :]\n",
    "\n",
    "print(f\"Number of riders who rode through high winds in Portland: {len(portland_wind)}, in Minneapolis: {len(minneapolis_wind)}, and in Boston: {len(boston_wind)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Micah's Description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
